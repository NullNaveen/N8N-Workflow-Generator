{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bf4937",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8239.64s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n",
      "Thu Oct 16 09:45:46 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla P100-PCIE-16GB           Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   36C    P0             31W /  250W |    7229MiB /  16384MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7fcb3186",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "833c564c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8901.47s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n",
      "Thu Oct 16 09:56:48 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla P100-PCIE-16GB           Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   54C    P0             34W /  250W |    9407MiB /  16384MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0e5ab862",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9034.88s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n",
      "\u001b[?1l\u001b>---------------------------------------+------------------------+----------\u001b[4h-\u001b[4l==========|;1HThu Oct 16 09:59:01 2025\u001b[1;75H2\u001b[3;19H2\u001b[24;80H\u001b[1;75H3\u001b[3;19H3\u001b[20;32H1\u001b[24;80H\u001b[1;75H4\u001b[3;19H4\u001b[24;80H\u001b[1;75H5\u001b[3;19H5\u001b[24;80H\u001b[1;75H6\u001b[3;19H6\u001b[24;80H\u001b[1;75H8\u001b[3;19H8\u001b[20;32H2\u001b[24;80H\u001b[1;75H9\u001b[3;19H9\u001b[20;32H1\u001b[24;80H\u001b[1;74H10\u001b[3;18H10\u001b[24;80H\u001b[1;75H1\u001b[3;19H1\u001b[24;80H\u001b[1;75H2\u001b[3;19H2\u001b[24;80H\u001b[1;75H3\u001b[3;19H3\u001b[20;32H2\u001b[24;80H\u001b[1;75H4\u001b[3;19H4\u001b[20;10H8\u001b[20;32H1\u001b[24;80H\u001b[1;75H5\u001b[3;19H5\u001b[24;80H\u001b[1;75H6\u001b[3;19H6\u001b[24;80H\u001b[1;75H7\u001b[3;19H7\u001b[20;32H2\u001b[24;80H\u001b[1;75H8\u001b[3;19H8\u001b[20;32H1\u001b[24;80H\u001b[1;75H9\u001b[3;19H9\u001b[24;80H\u001b[1;74H20\u001b[3;18H20\u001b[20;32H2\u001b[24;80H\u001b[1;75H1\u001b[3;19H1\u001b[24;80H\u001b[1;75H2\u001b[3;19H2\u001b[24;80H\u001b[1;75H3\u001b[3;19H3\u001b[20;32H1\u001b[24;80H\u001b[1;75H4\u001b[3;19H4\u001b[20;32H2\u001b[24;80H\u001b[1;75H5\u001b[3;19H5\u001b[24;80H\u001b[1;75H6\u001b[3;19H6\u001b[24;80H\u001b[1;75H7\u001b[3;19H8\u001b[20;32H1\u001b[24;80H\u001b[1;75H9\u001b[3;19H9\u001b[20;32H2\u001b[24;80H\u001b[1;74H30\u001b[3;18H30\u001b[20;32H1\u001b[24;80H\u001b[1;75H1\u001b[3;19H1\u001b[20;32H2\u001b[24;80H\u001b[1;75H2\u001b[3;19H2\u001b[24;80H\u001b[1;75H3\u001b[3;19H3\u001b[24;80H\u001b[1;75H4\u001b[3;19H4\u001b[20;32H1\u001b[24;80H\u001b[1;75H5\u001b[3;19H5\u001b[20;32H2\u001b[24;80H\u001b[1;75H6\u001b[3;19H6\u001b[20;32H1\u001b[24;80H\u001b[1;75H7\u001b[3;19H7\u001b[24;80H\u001b[1;75H8\u001b[3;19H8\u001b[20;32H2\u001b[24;80H\u001b[1;75H9\u001b[3;19H9\u001b[24;80H\u001b[1;74H40\u001b[3;18H40\u001b[24;80H\u001b[1;75H1\u001b[3;19H1\u001b[20;32H1\u001b[24;80H\u001b[1;75H2\u001b[3;19H2\u001b[20;32H2\u001b[24;80H\u001b[1;75H3\u001b[3;19H3\u001b[20;32H1\u001b[24;80H\u001b[1;75H4\u001b[3;19H4\u001b[24;80H\u001b[1;75H5\u001b[3;19H5\u001b[24;80H\u001b[1;75H6\u001b[3;19H6\u001b[24;80H\u001b[1;75H7\u001b[3;19H7\u001b[24;80H\u001b[1;75H9\u001b[3;19H9\u001b[20;32H2\u001b[24;80H\u001b[1;74H50\u001b[3;18H50\u001b[24;80H\u001b[1;75H1\u001b[3;19H1\u001b[20;32H1\u001b[24;80H\u001b[1;75H2\u001b[3;19H2\u001b[24;80H\u001b[1;75H3\u001b[3;19H3\u001b[20;10H7\u001b[24;80H\u001b[1;75H4\u001b[3;19H4\u001b[20;10H8\u001b[20;32H2\u001b[24;80H\u001b[1;75H5\u001b[3;19H5\u001b[20;10H7\u001b[24;80H\u001b[1;75H6\u001b[3;19H6\u001b[20;10H8\u001b[20;32H1\u001b[24;80H\u001b[1;75H7\u001b[3;19H7\u001b[20;10H7\u001b[24;80H\u001b[1;75H8\u001b[3;19H8\u001b[24;80H\u001b[1;75H9\u001b[3;19H9\u001b[20;32H2\u001b[24;80H\u001b[1;68H10:00:00\u001b[3;12H10:00:00\u001b[20;32H1\u001b[24;80H\u001b[1;75H1\u001b[3;19H1\u001b[24;80H\u001b[1;75H2\u001b[3;19H2\u001b[24;80H\u001b[1;75H3\u001b[3;19H3\u001b[24;80H\u001b[1;75H4\u001b[3;19H4\u001b[20;32H2\u001b[24;80H\u001b[1;75H5\u001b[3;19H5\u001b[20;32H1\u001b[24;80H\u001b[1;75H6\u001b[3;19H6\u001b[24;80H\u001b[1;75H7\u001b[3;19H7\u001b[24;80H\u001b[1;75H8\u001b[3;19H9\u001b[24;80H\u001b[1;74H10\u001b[3;18H10\u001b[20;32H2\u001b[24;80H\u001b[1;75H1\u001b[3;19H1\u001b[20;32H1\u001b[24;80H\u001b[1;75H2\u001b[3;19H2\u001b[24;80H\u001b[1;75H3\u001b[3;19H3\u001b[24;80H\u001b[1;75H4\u001b[3;19H4\u001b[24;80H\u001b[1;75H5\u001b[3;19H5\u001b[24;80H\u001b[1;75H6\u001b[3;19H6\u001b[24;80H\u001b[1;75H7\u001b[3;19H7\u001b[24;80H\u001b[24;1H\u001b[2J\u001b[?47l\u001b8"
     ]
    }
   ],
   "source": [
    "!watch -n 1 nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7cab38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "psutil.virtual_memory()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a9985f",
   "metadata": {},
   "source": [
    "# Test Progress Bar Widget\n",
    "\n",
    "Let's test if progress bars work in VS Code's Jupyter notebook interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d3237bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing tqdm progress bar...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|██████████| 100/100 [00:05<00:00, 19.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ tqdm test complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Test 1: Simple tqdm progress bar (most common library)\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "print(\"Testing tqdm progress bar...\")\n",
    "for i in tqdm(range(100), desc=\"Progress\"):\n",
    "    time.sleep(0.05)  # Simulate work\n",
    "print(\"✓ tqdm test complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cdfa62dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing tqdm.notebook progress bar (widget version)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5040d287ebb14450b2e82fee1d9c1ada",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Widget Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ tqdm.notebook test complete!\n"
     ]
    }
   ],
   "source": [
    "# Test 2: tqdm with notebook mode (uses ipywidgets)\n",
    "from tqdm.notebook import tqdm as tqdm_notebook\n",
    "import time\n",
    "\n",
    "print(\"Testing tqdm.notebook progress bar (widget version)...\")\n",
    "for i in tqdm_notebook(range(100), desc=\"Widget Progress\"):\n",
    "    time.sleep(0.05)  # Simulate work\n",
    "print(\"✓ tqdm.notebook test complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c18248c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing text-based progress bar...\n",
      "============================================================\n",
      "[████████████████████████████████████████] 100.0% | Step 50/50\n",
      "============================================================\n",
      "✓ Text-based progress test complete!\n"
     ]
    }
   ],
   "source": [
    "# Test 3: Text-based progress (no widgets, works everywhere)\n",
    "import time\n",
    "\n",
    "total_steps = 50\n",
    "print(\"Testing text-based progress bar...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i in range(total_steps):\n",
    "    # Calculate progress\n",
    "    progress_pct = ((i + 1) / total_steps) * 100\n",
    "    \n",
    "    # Create text progress bar\n",
    "    bar_length = 40\n",
    "    filled = int(bar_length * (i + 1) / total_steps)\n",
    "    bar = \"█\" * filled + \"░\" * (bar_length - filled)\n",
    "    \n",
    "    # Print progress (overwrite same line)\n",
    "    print(f\"\\r[{bar}] {progress_pct:.1f}% | Step {i+1}/{total_steps}\", end='', flush=True)\n",
    "    time.sleep(0.1)  # Simulate work\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✓ Text-based progress test complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "666408a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "🚀 SIMULATED TRAINING - Testing Full Progress Display\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "📊 Step 1/30 (3.3%)\n",
      "[█░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░]\n",
      "📉 Loss: 4.9000\n",
      "⏱️  Elapsed: 0.0 min | ETA: 0.1 min\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "📊 Step 5/30 (16.7%)\n",
      "[██████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░]\n",
      "📉 Loss: 4.5000\n",
      "⏱️  Elapsed: 0.0 min | ETA: 0.1 min\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "📊 Step 10/30 (33.3%)\n",
      "[█████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░]\n",
      "📉 Loss: 4.0000\n",
      "⏱️  Elapsed: 0.0 min | ETA: 0.1 min\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "📊 Step 15/30 (50.0%)\n",
      "[████████████████████░░░░░░░░░░░░░░░░░░░░]\n",
      "📉 Loss: 3.5000\n",
      "⏱️  Elapsed: 0.1 min | ETA: 0.1 min\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "📊 Step 20/30 (66.7%)\n",
      "[██████████████████████████░░░░░░░░░░░░░░]\n",
      "📉 Loss: 3.0000\n",
      "⏱️  Elapsed: 0.1 min | ETA: 0.0 min\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "📊 Step 25/30 (83.3%)\n",
      "[█████████████████████████████████░░░░░░░]\n",
      "📉 Loss: 2.5000\n",
      "⏱️  Elapsed: 0.1 min | ETA: 0.0 min\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "📊 Step 30/30 (100.0%)\n",
      "[████████████████████████████████████████]\n",
      "📉 Loss: 2.0000\n",
      "⏱️  Elapsed: 0.1 min | ETA: 0.0 min\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "✅ TRAINING SIMULATION COMPLETE!\n",
      "⏱️  Total Time: 0.1 minutes\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Test 4: Simulated training progress with all details\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"🚀 SIMULATED TRAINING - Testing Full Progress Display\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "total_steps = 30\n",
    "start_time = time.time()\n",
    "\n",
    "for step in range(1, total_steps + 1):\n",
    "    # Simulate some work\n",
    "    time.sleep(0.2)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    elapsed = time.time() - start_time\n",
    "    progress_pct = (step / total_steps) * 100\n",
    "    avg_time_per_step = elapsed / step\n",
    "    remaining_steps = total_steps - step\n",
    "    eta_seconds = avg_time_per_step * remaining_steps\n",
    "    eta_minutes = eta_seconds / 60\n",
    "    \n",
    "    # Fake loss (decreasing)\n",
    "    fake_loss = 5.0 - (step / total_steps) * 3.0\n",
    "    \n",
    "    # Create progress bar\n",
    "    bar_length = 40\n",
    "    filled = int(bar_length * step / total_steps)\n",
    "    bar = \"█\" * filled + \"░\" * (bar_length - filled)\n",
    "    \n",
    "    # Print update every 5 steps\n",
    "    if step % 5 == 0 or step == 1:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"📊 Step {step}/{total_steps} ({progress_pct:.1f}%)\")\n",
    "        print(f\"[{bar}]\")\n",
    "        print(f\"📉 Loss: {fake_loss:.4f}\")\n",
    "        print(f\"⏱️  Elapsed: {elapsed/60:.1f} min | ETA: {eta_minutes:.1f} min\")\n",
    "        print(f\"{'='*80}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✅ TRAINING SIMULATION COMPLETE!\")\n",
    "print(f\"⏱️  Total Time: {elapsed/60:.1f} minutes\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
