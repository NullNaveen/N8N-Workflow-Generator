{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13403772,"sourceType":"datasetVersion","datasetId":8506314}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"id":"fb5fa386","cell_type":"markdown","source":"# N8N Workflow Generator - Kaggle Training Notebook\n\nThis notebook is optimized for Kaggle. It will:\n- Check GPU availability\n- Install required packages\n- Load your uploaded dataset from `/kaggle/input/`\n- Train the model with a VS Code-friendly progress bar\n- Save checkpoints and final model to `/kaggle/working/`\n\n**How to use:**\n1. Upload your dataset as a Kaggle dataset and add it via 'Add Data'\n2. Run all cells in order\n3. Monitor progress in the output and `/kaggle/working/training_progress.log`\n4. Download the final model from the Output tab","metadata":{}},{"id":"cbf6d139","cell_type":"code","source":"# Check GPU availability\n!nvidia-smi","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T13:10:36.319424Z","iopub.execute_input":"2025-10-16T13:10:36.319929Z","iopub.status.idle":"2025-10-16T13:10:36.546927Z","shell.execute_reply.started":"2025-10-16T13:10:36.319904Z","shell.execute_reply":"2025-10-16T13:10:36.546038Z"}},"outputs":[],"execution_count":null},{"id":"7e025620-947a-442d-ae08-2b93fa5936a8","cell_type":"code","source":"!pip install transformers==4.44.2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T13:10:36.548448Z","iopub.execute_input":"2025-10-16T13:10:36.549123Z","iopub.status.idle":"2025-10-16T13:10:54.622178Z","shell.execute_reply.started":"2025-10-16T13:10:36.549099Z","shell.execute_reply":"2025-10-16T13:10:54.621465Z"}},"outputs":[],"execution_count":null},{"id":"44d5ea49","cell_type":"code","source":"# Install required packages\n!pip install -q datasets peft accelerate bitsandbytes scipy trl torch tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T13:10:54.623134Z","iopub.execute_input":"2025-10-16T13:10:54.623362Z","iopub.status.idle":"2025-10-16T13:12:30.93802Z","shell.execute_reply.started":"2025-10-16T13:10:54.62334Z","shell.execute_reply":"2025-10-16T13:12:30.937223Z"}},"outputs":[],"execution_count":null},{"id":"4aa6bbed","cell_type":"code","source":"# List available datasets in /kaggle/input/\nimport os\nprint('Datasets in /kaggle/input/:')\nfor item in os.listdir('/kaggle/input'):\n    print(' -', item)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T13:12:30.939661Z","iopub.execute_input":"2025-10-16T13:12:30.939921Z","iopub.status.idle":"2025-10-16T13:12:30.944864Z","shell.execute_reply.started":"2025-10-16T13:12:30.9399Z","shell.execute_reply":"2025-10-16T13:12:30.944205Z"}},"outputs":[],"execution_count":null},{"id":"ac555dca","cell_type":"code","source":"# Load your dataset (update path if needed)\nimport json\nfrom datasets import Dataset\ndataset_path = '/kaggle/input/testingdata/dataset.jsonl'  # Change if needed\nformatted_data = []\nwith open(dataset_path, 'r', encoding='utf-8') as f:\n    for line in f:\n        if line.strip():\n            item = json.loads(line.strip())\n            workflow_str = json.dumps(item['workflow']) if isinstance(item['workflow'], dict) else item['workflow']\n            formatted_data.append({\n                'text': f'''<|system|>\nYou are an n8n workflow generator. Convert natural language descriptions into valid n8n workflow JSON.\n<|user|>\n{item['prompt']}\n<|assistant|>\n{workflow_str}'''\n            })\ntrain_dataset = Dataset.from_list(formatted_data)\nprint(f'Loaded {len(train_dataset)} examples')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T13:12:30.945625Z","iopub.execute_input":"2025-10-16T13:12:30.945875Z","iopub.status.idle":"2025-10-16T13:12:33.053303Z","shell.execute_reply.started":"2025-10-16T13:12:30.945852Z","shell.execute_reply":"2025-10-16T13:12:33.052619Z"}},"outputs":[],"execution_count":null},{"id":"02c5e6ff-e862-4d52-a5c0-b9e37a623434","cell_type":"code","source":"# Load model and tokenizer (after restart)\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n\nmodel_name = 'mistralai/Mistral-7B-Instruct-v0.2'\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True, \n    bnb_4bit_quant_type='nf4', \n    bnb_4bit_compute_dtype=torch.bfloat16, \n    bnb_4bit_use_double_quant=True\n)\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = 'right'\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name, \n    quantization_config=bnb_config, \n    device_map='auto'\n)\nmodel.config.use_cache = False\nmodel = prepare_model_for_kbit_training(model)\nprint('âœ… Model loaded successfully!')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T13:12:33.054058Z","iopub.execute_input":"2025-10-16T13:12:33.05425Z","iopub.status.idle":"2025-10-16T13:15:46.57538Z","shell.execute_reply.started":"2025-10-16T13:12:33.054234Z","shell.execute_reply":"2025-10-16T13:15:46.574355Z"}},"outputs":[],"execution_count":null},{"id":"3537de26","cell_type":"code","source":"# Configure LoRA adapters\nfrom peft import LoraConfig, get_peft_model\nlora_config = LoraConfig(r=16, lora_alpha=32, target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj'], lora_dropout=0.05, bias='none', task_type='CAUSAL_LM')\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T13:15:46.576232Z","iopub.execute_input":"2025-10-16T13:15:46.577176Z","iopub.status.idle":"2025-10-16T13:15:46.934714Z","shell.execute_reply.started":"2025-10-16T13:15:46.577154Z","shell.execute_reply":"2025-10-16T13:15:46.934016Z"}},"outputs":[],"execution_count":null},{"id":"efc0499d","cell_type":"code","source":"# Training arguments\nfrom transformers import TrainingArguments\ntraining_args = TrainingArguments(\n    output_dir='/kaggle/working/n8n-workflow-generator',\n    num_train_epochs=3,\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=8,\n    learning_rate=2e-4,\n    fp16=True,\n    save_strategy='steps',\n    save_steps=25,\n    save_total_limit=3,\n    logging_steps=5,\n    warmup_steps=100,\n    optim='paged_adamw_8bit',\n    max_grad_norm=0.3,\n    lr_scheduler_type='cosine',\n    report_to='none',\n    logging_first_step=True,\n    disable_tqdm=False,\n    gradient_checkpointing=True,\n)\nprint('Training arguments configured!')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T13:15:46.935381Z","iopub.execute_input":"2025-10-16T13:15:46.935603Z","iopub.status.idle":"2025-10-16T13:15:46.998872Z","shell.execute_reply.started":"2025-10-16T13:15:46.935579Z","shell.execute_reply":"2025-10-16T13:15:46.998258Z"}},"outputs":[],"execution_count":null},{"id":"14d8f0aa","cell_type":"code","source":"# Training cell with VS Code-friendly progress bar and log file\nfrom transformers import Trainer, DataCollatorForLanguageModeling, TrainerCallback\nimport time\nimport os\n\nclass TextProgressCallback(TrainerCallback):\n    def __init__(self, total_steps, epochs, log_path):\n        self.total_steps = total_steps\n        self.epochs = epochs\n        self.start_time = None\n        self.last_logged_step = -1\n        self.log_path = log_path\n        if os.path.exists(log_path):\n            os.remove(log_path)\n    def _progress_bar(self, current, total, width=40):\n        filled = int(width * current / max(1, total))\n        return 'â–ˆ' * filled + 'â–‘' * (width - filled)\n    def _log(self, msg):\n        with open(self.log_path, 'a', encoding='utf-8') as f:\n            f.write(msg + '\\n')\n    def on_train_begin(self, args, state, control, **kwargs):\n        self.start_time = time.time()\n        self._log('TRAINING STARTED')\n    def on_log(self, args, state, control, logs=None, **kwargs):\n        if not logs:\n            return\n        current_step = state.global_step\n        if current_step == self.last_logged_step:\n            return\n        self.last_logged_step = current_step\n        elapsed = time.time() - self.start_time\n        total = self.total_steps if self.total_steps else max(1, current_step)\n        progress_pct = (current_step / total) * 100\n        if current_step > 0:\n            avg_time_per_step = elapsed / current_step\n            remaining_steps = max(0, total - current_step)\n            eta_minutes = (avg_time_per_step * remaining_steps) / 60\n        else:\n            eta_minutes = 0\n        bar = self._progress_bar(current_step, total)\n        loss = logs.get('loss')\n        msg = f'Step {current_step}/{total} ({progress_pct:.1f}%) | Loss: {loss:.4f} | Elapsed: {elapsed/60:.1f} min | ETA: {eta_minutes:.1f} min'\n        print('\\n' + '='*80)\n        print(f'ðŸ“Š {msg}')\n        print(f'[{bar}]')\n        print('='*80)\n        self._log(msg)\n    def on_save(self, args, state, control, **kwargs):\n        elapsed = time.time() - self.start_time if self.start_time else 0\n        ckpt = f'checkpoint-{state.global_step}'\n        msg = f'Checkpoint saved: {ckpt} | Elapsed: {elapsed/60:.1f} min'\n        print(msg)\n        self._log(msg)\n    def on_train_end(self, args, state, control, **kwargs):\n        total_time = time.time() - self.start_time if self.start_time else 0\n        msg = f'TRAINING COMPLETE! Total Time: {total_time/60:.1f} min'\n        print(msg)\n        self._log(msg)\n\ndef tokenize_function(examples):\n    return tokenizer(examples['text'], truncation=True, max_length=2048, padding='max_length')\n\nprint('Tokenizing dataset...')\ntokenized_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=['text'])\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\ncheckpoint_dir = '/content/n8n-workflow-generator'\nresume_from_checkpoint = None\nif os.path.exists(checkpoint_dir):\n    checkpoints = [d for d in os.listdir(checkpoint_dir) if d.startswith('checkpoint-')]\n    if checkpoints:\n        latest_checkpoint = sorted(checkpoints, key=lambda x: int(x.split('-')[1]))[-1]\n        resume_from_checkpoint = os.path.join(checkpoint_dir, latest_checkpoint)\n        print(f'Found checkpoint: {latest_checkpoint}, resuming...')\ntotal_steps = len(train_dataset) * training_args.num_train_epochs // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps)\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset,\n    data_collator=data_collator,\n    callbacks=[TextProgressCallback(total_steps, training_args.num_train_epochs, '/content/training_progress.log')],\n)\ntrainer.train(resume_from_checkpoint=resume_from_checkpoint)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T13:15:46.999588Z","iopub.execute_input":"2025-10-16T13:15:46.999895Z","execution_failed":"2025-10-16T13:23:02.959Z"}},"outputs":[],"execution_count":null},{"id":"1fa47c60","cell_type":"code","source":"# Save final model\noutput_dir = '/kaggle/working/n8n-workflow-generator-final'\ntrainer.model.save_pretrained(output_dir)\ntokenizer.save_pretrained(output_dir)\nprint(f'Model saved to {output_dir}')","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-16T13:23:02.959Z"}},"outputs":[],"execution_count":null}]}