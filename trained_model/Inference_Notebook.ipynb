{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "655e79f6",
   "metadata": {},
   "source": [
    "# N8N Workflow Generator - Inference Notebook\n",
    "\n",
    "Use the trained LoRA model with Mistral-7B to generate n8n workflows from natural language descriptions.\n",
    "\n",
    "## Model Information:\n",
    "- **Base Model**: mistralai/Mistral-7B-Instruct-v0.2\n",
    "- **Training Method**: LoRA (Low-Rank Adaptation)\n",
    "- **LoRA Rank**: 16\n",
    "- **Dataset**: n8n workflow examples with natural language descriptions\n",
    "- **Training Status**: ‚úÖ Successfully Trained\n",
    "- **Model Location**: `./trained_model/`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9002ba",
   "metadata": {},
   "source": [
    "## Step 1: Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2e24e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (this may take a few minutes)\n",
    "!pip install -q transformers peft accelerate bitsandbytes torch\n",
    "print('‚úÖ Packages installed successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed72bd5",
   "metadata": {},
   "source": [
    "## Step 2: Import and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c1de87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "print('‚úÖ All imports successful!')\n",
    "print(f'PyTorch version: {torch.__version__}')\n",
    "print(f'GPU available: {torch.cuda.is_available()}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f90acc",
   "metadata": {},
   "source": [
    "## Step 3: Load the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85af2ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the trained model\n",
    "model_dir = Path('./trained_model')\n",
    "base_model = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "print(f\"üîß Loading model from {model_dir}...\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'right'\n",
    "print(\"‚úÖ Tokenizer loaded\")\n",
    "\n",
    "# Load base model with 4-bit quantization\n",
    "try:\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type='nf4',\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=True\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    print(\"‚úÖ Base model loaded with 4-bit quantization\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Quantization failed: {e}\")\n",
    "    print(\"Loading with fp16 instead...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    print(\"‚úÖ Base model loaded with fp16\")\n",
    "\n",
    "# Load LoRA adapter\n",
    "model = PeftModel.from_pretrained(model, model_dir)\n",
    "model.eval()\n",
    "print(\"‚úÖ LoRA adapter loaded\")\n",
    "print(\"\\n‚úÖ Model fully loaded and ready for inference!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486d88bc",
   "metadata": {},
   "source": [
    "## Step 4: Generate n8n Workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086e4f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_workflow(prompt, max_length=2048, temperature=0.7, top_p=0.9):\n",
    "    \"\"\"\n",
    "    Generate an n8n workflow from natural language description\n",
    "    \n",
    "    Args:\n",
    "        prompt: Natural language description\n",
    "        max_length: Maximum tokens to generate\n",
    "        temperature: Sampling temperature (higher = more creative)\n",
    "        top_p: Nucleus sampling parameter\n",
    "    \n",
    "    Returns:\n",
    "        Generated workflow (text)\n",
    "    \"\"\"\n",
    "    # Format prompt in training style\n",
    "    formatted_prompt = f\"\"\"<|system|>\n",
    "You are an n8n workflow generator. Convert natural language descriptions into valid n8n workflow JSON.\n",
    "<|user|>\n",
    "{prompt}\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(\n",
    "        formatted_prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True\n",
    "    ).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract assistant response\n",
    "    if \"<|assistant|>\" in response:\n",
    "        response = response.split(\"<|assistant|>\")[-1].strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "print(\"‚úÖ Generation function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad05338",
   "metadata": {},
   "source": [
    "## Example 1: Webhook + Email Notification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2beb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = \"Create a workflow that receives data from a webhook, processes it, and sends an email notification\"\n",
    "\n",
    "print(f\"üìù Prompt: {prompt1}\\n\")\n",
    "print(\"üîÑ Generating workflow...\\n\")\n",
    "\n",
    "workflow1 = generate_workflow(prompt1)\n",
    "print(\"üìã Generated Workflow:\")\n",
    "print(workflow1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa8a0dc",
   "metadata": {},
   "source": [
    "## Example 2: API Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6079ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt2 = \"Build a workflow that fetches data from an API every hour and stores it in a database\"\n",
    "\n",
    "print(f\"üìù Prompt: {prompt2}\\n\")\n",
    "print(\"üîÑ Generating workflow...\\n\")\n",
    "\n",
    "workflow2 = generate_workflow(prompt2)\n",
    "print(\"üìã Generated Workflow:\")\n",
    "print(workflow2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decbf1ff",
   "metadata": {},
   "source": [
    "## Example 3: Slack Message Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e728df",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt3 = \"Generate a workflow that listens for Slack messages and logs them to a file\"\n",
    "\n",
    "print(f\"üìù Prompt: {prompt3}\\n\")\n",
    "print(\"üîÑ Generating workflow...\\n\")\n",
    "\n",
    "workflow3 = generate_workflow(prompt3)\n",
    "print(\"üìã Generated Workflow:\")\n",
    "print(workflow3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9b32ae",
   "metadata": {},
   "source": [
    "## Try Your Own Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbc9aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéØ Try your own workflow description here!\n",
    "custom_prompt = \"YOUR WORKFLOW DESCRIPTION HERE\"\n",
    "\n",
    "print(f\"üìù Your Prompt: {custom_prompt}\\n\")\n",
    "print(\"üîÑ Generating workflow...\\n\")\n",
    "\n",
    "custom_workflow = generate_workflow(custom_prompt)\n",
    "print(\"üìã Generated Workflow:\")\n",
    "print(custom_workflow)\n",
    "\n",
    "# Try to parse as JSON\n",
    "try:\n",
    "    json_start = custom_workflow.find('{')\n",
    "    json_end = custom_workflow.rfind('}') + 1\n",
    "    if json_start >= 0 and json_end > json_start:\n",
    "        json_str = custom_workflow[json_start:json_end]\n",
    "        workflow_json = json.loads(json_str)\n",
    "        print(\"\\n‚úÖ Valid JSON detected:\")\n",
    "        print(json.dumps(workflow_json, indent=2))\n",
    "except:\n",
    "    print(\"\\n‚ö†Ô∏è Could not parse JSON from response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45433e5e",
   "metadata": {},
   "source": [
    "## Model Performance Notes:\n",
    "\n",
    "- The model was trained on n8n workflow examples with LoRA (parameter-efficient fine-tuning)\n",
    "- It uses a Mistral-7B instruction-tuned base model for better instruction following\n",
    "- Generation quality can be improved by:\n",
    "  - Adjusting `temperature` (lower = more deterministic, higher = more creative)\n",
    "  - Adjusting `top_p` (nucleus sampling for diversity)\n",
    "  - Providing more specific, detailed prompts\n",
    "  - Post-processing the JSON output if needed"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
