{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "470d7b26",
   "metadata": {},
   "source": [
    "# N8N Workflow Generator - Colab Training Notebook\n",
    "\n",
    "This notebook is optimized for Google Colab. It will:\n",
    "- Check GPU availability\n",
    "- Install required packages\n",
    "- Upload your dataset from local or Google Drive\n",
    "- Train the model with a VS Code-friendly progress bar\n",
    "- Save checkpoints and final model to `/content/`\n",
    "\n",
    "**How to use:**\n",
    "1. Upload your dataset (use the upload cell or mount Google Drive)\n",
    "2. Run all cells in order\n",
    "3. Monitor progress in the output and `/content/training_progress.log`\n",
    "4. Download the final model from the Files sidebar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36bdd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi || echo 'No GPU found'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c925a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers datasets peft accelerate bitsandbytes scipy trl torch tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18c78e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload dataset from local\n",
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "dataset_path = list(uploaded.keys())[0]  # Use the first uploaded file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef088586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataset\n",
    "import json\n",
    "from datasets import Dataset\n",
    "formatted_data = []\n",
    "with open(dataset_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        if line.strip():\n",
    "            item = json.loads(line.strip())\n",
    "            workflow_str = json.dumps(item['workflow']) if isinstance(item['workflow'], dict) else item['workflow']\n",
    "            formatted_data.append({\n",
    "                'text': f'''<|system|>\n",
    "You are an n8n workflow generator. Convert natural language descriptions into valid n8n workflow JSON.\n",
    "<|user|>\n",
    "{item['prompt']}\n",
    "<|assistant|>\n",
    "{workflow_str}'''\n",
    "            })\n",
    "train_dataset = Dataset.from_list(formatted_data)\n",
    "print(f'Loaded {len(train_dataset)} examples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f99fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and tokenizer\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "model_name = 'mistralai/Mistral-7B-Instruct-v0.2'\n",
    "bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type='nf4', bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_use_double_quant=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'right'\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config, device_map='auto', trust_remote_code=True)\n",
    "model.config.use_cache = False\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "print('Model loaded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b5b42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure LoRA adapters\n",
    "from peft import LoraConfig, get_peft_model\n",
    "lora_config = LoraConfig(r=16, lora_alpha=32, target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj'], lora_dropout=0.05, bias='none', task_type='CAUSAL_LM')\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07069ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "from transformers import TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='/content/n8n-workflow-generator',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    save_strategy='steps',\n",
    "    save_steps=25,\n",
    "    save_total_limit=3,\n",
    "    logging_steps=5,\n",
    "    warmup_steps=100,\n",
    "    optim='paged_adamw_8bit',\n",
    "    max_grad_norm=0.3,\n",
    "    lr_scheduler_type='cosine',\n",
    "    report_to='none',\n",
    "    logging_first_step=True,\n",
    "    disable_tqdm=False,\n",
    "    gradient_checkpointing=True,\n",
    ")\n",
    "print('Training arguments configured!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f673cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training cell with VS Code-friendly progress bar and log file\n",
    "from transformers import Trainer, DataCollatorForLanguageModeling, TrainerCallback\n",
    "import time\n",
    "import os\n",
    "class TextProgressCallback(TrainerCallback):\n",
    "    def __init__(self, total_steps, epochs, log_path):\n",
    "        self.total_steps = total_steps\n",
    "        self.epochs = epochs\n",
    "        self.start_time = None\n",
    "        self.last_logged_step = -1\n",
    "        self.log_path = log_path\n",
    "        if os.path.exists(log_path):\n",
    "            os.remove(log_path)\n",
    "    def _progress_bar(self, current, total, width=40):\n",
    "        filled = int(width * current / max(1, total))\n",
    "        return 'â–ˆ' * filled + 'â–‘' * (width - filled)\n",
    "    def _log(self, msg):\n",
    "        with open(self.log_path, 'a', encoding='utf-8') as f:\n",
    "            f.write(msg + '\n",
    "')\n",
    "    def on_train_begin(self, args, state, control, **kwargs):\n",
    "        self.start_time = time.time()\n",
    "        self._log('TRAINING STARTED')\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if not logs:\n",
    "        current_step = state.global_step\n",
    "        if current_step == self.last_logged_step:\n",
    "        self.last_logged_step = current_step\n",
    "        elapsed = time.time() - self.start_time\n",
    "        total = self.total_steps if self.total_steps else max(1, current_step)\n",
    "        progress_pct = (current_step / total) * 100\n",
    "        if current_step > 0:\n",
    "        bar = self._progress_bar(current_step, total)\n",
    "        loss = logs.get('loss')\n",
    "        msg = f'Step {current_step}/{total} ({progress_pct:.1f}%) | Loss: {loss:.4f} | Elapsed: {elapsed/60:.1f} min | ETA: {eta_minutes:.1f} min'\n",
    "        print(f'\n",
    "{'='*80}')\n",
    "        print(f'ðŸ“Š {msg}')\n",
    "        print(f'[{bar}]')\n",
    "        print(f\n",
    ")\n",
    "        self._log(msg)\n",
    "    def on_save(self, args, state, control, **kwargs):\n",
    "        elapsed = time.time() - self.start_time if self.start_time else 0\n",
    "        ckpt = f'checkpoint-{state.global_step}'\n",
    "        msg = f'Checkpoint saved: {ckpt} | Elapsed: {elapsed/60:.1f} min'\n",
    "        print(msg)\n",
    "        self._log(msg)\n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        total_time = time.time() - self.start_time if self.start_time else 0\n",
    "        msg = f'TRAINING COMPLETE! Total Time: {total_time/60:.1f} min'\n",
    "        print(msg)\n",
    "        self._log(msg)\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, max_length=2048, padding='max_length')\n",
    "print('Tokenizing dataset...')\n",
    "tokenized_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=['text'])\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "checkpoint_dir = '/content/n8n-workflow-generator'\n",
    "resume_from_checkpoint = None\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    checkpoints = [d for d in os.listdir(checkpoint_dir) if d.startswith('checkpoint-')]\n",
    "    if checkpoints:\n",
    "total_steps = len(train_dataset) * training_args.num_train_epochs // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[TextProgressCallback(total_steps, training_args.num_train_epochs, '/content/training_progress.log')],\n",
    ")\n",
    "trainer.train(resume_from_checkpoint=resume_from_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b582fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "output_dir = '/content/n8n-workflow-generator-final'\n",
    "trainer.model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "print(f'Model saved to {output_dir}')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
