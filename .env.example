# Local LLM inference endpoint used by the app
# Change if your server runs on a different host/port
LOCAL_INFER_URL=http://127.0.0.1:8000/generate
